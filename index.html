<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Raphaël Berthier</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Raphaël Berthier</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Raphaël Berthier</h1>
</div>
<table class="imgtable"><tr><td>
<img src="photo2.jpg" alt="" width="350px" />&nbsp;</td>
<td align="left"><h2>Briefly</h2>
<p>I am a postdoc under the supervision of <a href="https://people.epfl.ch/emmanuel.abbe">Emmanuel Abbe</a> and <a href="https://web.stanford.edu/~montanar/">Andrea Montanari</a>. </p>
<p>Before that, I was a PhD student under the supervision of <a href="http://www.di.ens.fr/~fbach/">Francis Bach</a> and <a href="http://pierre.gaillard.me/">Pierre Gaillard</a>. My PhD thesis is a parallel study of optimization and gossip algorithms. The best way to learn about this is to look at the summary at the beginning of the <a href="thesis.pdf">manuscript</a>. </p>
<p>Here is a short <a href="cv_english.pdf">CV</a>.</p>
<h2>Contact</h2>
<ul>
<li><p>E-mail: raphael [dot] berthier [at] epfl [dot] ch <br />
In the perspective of making research more self-critical, I welcome any comments, reviews, critiques, or suggestions on my work.</p>
</li>
<li><p>Physical address: <a href="https://plan.epfl.ch/?room=%3DMA%20C2%20553&amp;dim_floor=2&amp;lang=en&amp;dim_lang=en&amp;tree_groups=centres_nevralgiques%2Cacces%2Cmobilite_reduite%2Censeignement%2Ccommerces_et_services%2Cvehicules%2Cinfrastructure_plan_grp&amp;tree_group_layers_centres_nevralgiques=information_epfl%2Cguichet_etudiants&amp;tree_group_layers_acces=metro&amp;tree_group_layers_mobilite_reduite=&amp;tree_group_layers_enseignement=&amp;tree_group_layers_commerces_et_services=&amp;tree_group_layers_vehicules=&amp;tree_group_layers_infrastructure_plan_grp=batiments_query_plan&amp;baselayer_ref=grp_backgrounds&amp;map_x=2533276&amp;map_y=1152345&amp;map_zoom=10">EPFL, MA C2 553 (batiment MA), Lausanne, Switzerland</a>.</p>
</li>
</ul>
</td></tr></table>
<h2>Publications and Preprints</h2>
<ul>
<li><p>R. Berthier. <b>Incremental Learning in Diagonal Linear Networks</b>, 2022, preprint. <br />[<a href="https://arxiv.org/abs/2208.14673">arXiv</a>] <span class="toggle-trigger">[Show Abstract]<span> 
<div class="toggle-wrap">
<b>Abstract:</b> Diagonal linear networks (DLNs) are a toy simplification of artificial neural networks; they consist in a quadratic reparametrization of linear regression inducing a sparse implicit regularization. In this paper, we describe the trajectory of the gradient flow of DLNs in the limit of small initialization. We show that incremental learning is effectively performed in the limit: coordinates are successively activated, while the iterate is the minimizer of the loss constrained to have support on the active coordinates only. This shows that the sparse implicit regularization of DLNs decreases with time. This work is restricted to the underparametrized regime with anti-correlated features for technical reasons. 
</div></p>
</li>
</ul>
<ul>
<li><p>R. Berthier, M. B. Li. <b>Acceleration of Gossip Algorithms through the Euler-Poisson-Darboux Equation</b>, 2022, accepted for publication in <i>Information and Inference: a Journal of the IMA</i>. <br />[<a href="https://arxiv.org/abs/2202.10742">arXiv</a>] <span class="toggle-trigger">[Show Abstract]<span> 
<div class="toggle-wrap">
<b>Abstract:</b> Gossip algorithms and their accelerated versions have been studied exclusively in discrete time on graphs. In this work, we take a different approach, and consider the scaling limit of gossip algorithms in both large graphs and large number of iterations. These limits lead to well-known partial differential equations (PDEs) with insightful properties. On lattices, we prove that the non-accelerated gossip algorithm of Boyd et al. [2006] converges to the heat equation, and the accelerated Jacobi polynomial iteration of Berthier et al. [2020] converges to the Euler-Poisson-Darboux (EPD) equation - a damped wave equation. Remarkably, with appropriate parameters, the fundamental solution of the EPD equation has the ideal gossip behaviour: a uniform density over an ellipsoid, whose radius increases at a rate proportional to t - the fastest possible rate for locally communicating gossip algorithms. This is in contrast with the heat equation where the density spreads on a typical scale of <img class="eq" src="eqs/2909845988863612064-130.png" alt="sqrt{t}" style="vertical-align: -4px" />. Additionally, we provide simulations demonstrating that the gossip algorithms are accurately approximated by their limiting PDEs. 
</div></p>
</li>
</ul>
<ul>
<li><p>C. Gerbelot, R. Berthier. <b>Graph-based Approximate Message Passing Iterations</b>, 2021, preprint. <br />[<a href="https://arxiv.org/abs/2109.11905">arXiv</a>] <span class="toggle-trigger">[Show Abstract]<span> 
<div class="toggle-wrap">
<b>Abstract:</b> Approximate-message passing (AMP) algorithms have become an important element of high-dimensional statistical inference, mostly due to their adaptability and concentration properties, the state evolution (SE) equations. This is demonstrated by the growing number of new iterations proposed for increasingly complex problems, ranging from multi-layer inference to low-rank matrix estimation with elaborate priors. In this paper, we address the following questions: is there a structure underlying all AMP iterations that unifies them in a common framework? Can we use such a structure to give a modular proof of state evolution equations, adaptable to new AMP iterations without reproducing each time the full argument ? We propose an answer to both questions, showing that AMP instances can be generically indexed by an oriented graph. This enables to give a unified interpretation of these iterations, independent from the problem they solve, and a way of composing them arbitrarily. We then show that all AMP iterations indexed by such a graph admit rigorous SE equations, extending the reach of previous proofs, and proving a number of recent heuristic derivations of those equations. Our proof naturally includes non-separable functions and we show how existing refinements, such as spatial coupling or matrix-valued variables, can be combined with our framework. 
</div></p>
</li>
</ul>
<ul>
<li><p>M. Even, R. Berthier, F. Bach, N. Flammarion, P. Gaillard, H. Hendrikx, L. Massoulié, A. Taylor. <b>A Continuized View on Nesterov Acceleration for Stochastic Gradient Descent and Randomized Gossip</b>, 2021, oustanding paper award and oral at <i>Advances in Neural Information Processing Systems (NeurIPS)</i>. <br />[<a href="https://proceedings.neurips.cc/paper/2021/hash/ec26fc2eb2b75aece19c70392dc744c2-Abstract.html">conference version</a>, <a href="https://arxiv.org/abs/2106.07644">arXiv</a>] <span class="toggle-trigger">[Show Abstract]<span> 
<div class="toggle-wrap">
<b>Abstract:</b> We introduce the continuized Nesterov acceleration, a close variant of Nesterov acceleration whose variables are indexed by a continuous time parameter. The two variables continuously mix following a linear ordinary differential equation and take gradient steps at random times. This continuized variant benefits from the best of the continuous and the discrete frameworks: as a continuous process, one can use differential calculus to analyze convergence and obtain analytical expressions for the parameters; and a discretization of the continuized process can be computed exactly with convergence rates similar to those of Nesterov original acceleration. We show that the discretization has the same structure as Nesterov acceleration, but with random parameters. We provide continuized Nesterov acceleration under deterministic as well as stochastic gradients, with either additive or multiplicative noise. Finally, using our continuized framework and expressing the gossip averaging problem as the stochastic minimization of a certain energy function, we provide the first rigorous acceleration of asynchronous gossip algorithms. 
</div></p>
</li>
</ul>
<ul>
<li><p>R. Berthier, F. Bach, P. Gaillard. <b>Tight Nonparametric Convergence Rates for Stochastic Gradient Descent under the Noiseless Linear Model</b>, 2020, <i>Advances in Neural Information Processing Systems (NeurIPS)</i>. <br />[<a href="https://papers.nips.cc/paper/2020/hash/1b33d16fc562464579b7199ca3114982-Abstract.html">conference version</a>, <a href="https://hal.archives-ouvertes.fr/hal-02866755">hal</a>, <a href="https://arxiv.org/abs/2006.08212">arXiv</a>] <span class="toggle-trigger">[Show Abstract]<span> 
<div class="toggle-wrap">
<b>Abstract:</b> In the context of statistical supervised learning, the noiseless linear model assumes that there exists a deterministic linear relation <img class="eq" src="eqs/9200006626087324140-130.png" alt="Y = langle theta_*, X rangle" style="vertical-align: -5px" /> between the random output <img class="eq" src="eqs/11392034264-130.png" alt="Y" style="vertical-align: -0px" /> and the random feature vector <img class="eq" src="eqs/1230657311937148082-130.png" alt="Phi(U)" style="vertical-align: -5px" />, a potentially non-linear transformation of the inputs <img class="eq" src="eqs/10880032724-130.png" alt="U" style="vertical-align: -1px" />. We analyze the convergence of single-pass, fixed step-size stochastic gradient descent on the least-square risk under this model. The convergence of the iterates to the optimum <img class="eq" src="eqs/1854661537855120535-130.png" alt="theta_*" style="vertical-align: -3px" /> and the decay of the generalization error follow polynomial convergence rates with exponents that both depend on the regularities of the optimum <img class="eq" src="eqs/1854661537855120535-130.png" alt="theta_*" style="vertical-align: -3px" /> and of the feature vectors <img class="eq" src="eqs/1230657311937148082-130.png" alt="Phi(U)" style="vertical-align: -5px" />. We interpret our result in the reproducing kernel Hilbert space framework; as a special case, we analyze an online algorithm for estimating a real function on the unit interval from the noiseless observation of its value at randomly sampled points. The convergence depends on the Sobolev smoothness of the function and of a chosen kernel. Finally, we apply our analysis beyond the supervised learning setting to obtain convergence rates for the averaging process (a.k.a. gossip algorithm) on a graph depending on its spectral dimension. 
</div></p>
</li>
</ul>
<ul>
<li><p>R. Berthier, F. Bach, P. Gaillard. <b>Accelerated Gossip in Networks of Given Dimension using Jacobi Polynomial Iterations</b>, 2020, <i>SIAM Journal on Mathematics of Data Science (SIMODS)</i>. <br />[<a href="papers/accelerated_gossip_merged.pdf">journal</a>, <a href="https://hal.archives-ouvertes.fr/hal-01797016/">hal</a>, <a href="https://arxiv.org/abs/1805.08531">arXiv</a>] <span class="toggle-trigger">[Show Abstract]<span> 
<div class="toggle-wrap">
<b>Abstract:</b> Consider a network of agents connected by communication links, where each agent holds a real value. The gossip problem consists in estimating the average of the values diffused in the network in a distributed manner. We develop a method solving the gossip problem that depends only on the spectral dimension of the network, that is, in the communication network set-up, the dimension of the space in which the agents live. This contrasts with previous work that required the spectral gap of the network as a parameter, or suffered from slow mixing. Our method shows an important improvement over existing algorithms in the non-asymptotic regime, i.e., when the values are far from being fully mixed in the network. Our approach stems from a polynomial-based point of view on gossip algorithms, as well as an approximation of the spectral measure of the graphs with a Jacobi measure. We show the power of the approach with simulations on various graphs, and with performance guarantees on graphs of known spectral dimension, such as grids and random percolation bonds. An extension of this work to distributed Laplacian solvers is discussed. As a side result, we also use the polynomial-based point of view to show the convergence of the message passing algorithm for gossip of Moallemi & Van Roy on regular graphs. The explicit computation of the rate of the convergence shows that message passing has a slow rate of convergence on graphs with small spectral gap. 
</div></p>
</li>
</ul>
<ul>
<li><p>R. Berthier, A. Montanari, P.-M. Nguyen. <b>State Evolution for Approximate Message Passing with Non-Separable Functions</b>, 2017, <i>Information and Inference: a Journal of the IMA</i>. <br />[<a href="https://doi.org/10.1093/imaiai/iay021">journal</a>, <a href="https://arxiv.org/abs/1708.03950">arXiv</a>] <span class="toggle-trigger">[Show Abstract]<span> 
<div class="toggle-wrap">
<b>Abstract:</b> Given a high-dimensional data matrix <img class="eq" src="eqs/4536482735675098617-130.png" alt="A in {rm I!R}^{n times m}" style="vertical-align: -1px" />, Approximate Message Passing (AMP) algorithms construct sequences of vectors <img class="eq" src="eqs/4353495100622012351-130.png" alt="u^t in {rm I!R}^n" style="vertical-align: -1px" />, <img class="eq" src="eqs/1188365746961071671-130.png" alt="v^t in {rm I!R}^m" style="vertical-align: -1px" />, indexed by <img class="eq" src="eqs/4718066532391173994-130.png" alt="t in {0,1,2,dots }" style="vertical-align: -5px" /> by iteratively applying <img class="eq" src="eqs/8320025024-130.png" alt="A" style="vertical-align: -0px" /> or <img class="eq" src="eqs/593367982112446592-130.png" alt="A^T" style="vertical-align: -0px" />, and suitable non-linear functions, which depend on the specific application. Special instances of this approach have been developed &ndash;among other applications&ndash; for compressed sensing reconstruction, robust regression, Bayesian estimation, low-rank matrix recovery, phase retrieval, and community detection in graphs. For certain classes of random matrices <img class="eq" src="eqs/8320025024-130.png" alt="A" style="vertical-align: -0px" />, AMP admits an asymptotically exact description in the high-dimensional limit <img class="eq" src="eqs/3244372985665555957-130.png" alt="m,nto infty" style="vertical-align: -4px" />, which goes under the name of &lsquo;state evolution.&rsquo;
Earlier work established state evolution for separable non-linearities (under certain regularity conditions). Nevertheless, empirical work demonstrated several important applications that require non-separable functions. In this paper we generalize state evolution to Lipschitz continuous non-separable nonlinearities, for Gaussian matrices <img class="eq" src="eqs/8320025024-130.png" alt="A" style="vertical-align: -0px" />. Our proof makes use of Bolthausen's conditioning technique along with several approximation arguments. In particular, we introduce a modified algorithm (called LAMP for Long AMP) which is of independent interest. 
</div></p>
</li>
</ul>
<h2>PhD thesis</h2>
<ul>
<li><p>R. Berthier. <b>Analysis and Acceleration of Gradient Descents and Gossip Algorithms</b>, 2021, <a href="thesis.pdf">manuscript</a>.</p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2022-09-20 16:28:47 CEST, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
<script>
$(document).ready(function() {
    $(".toggle-trigger").click(function() {
        $(this).parent().nextAll('.toggle-wrap').first().slideToggle('slow');
    });
});
</script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-128753599-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
