<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Raphaël Berthier</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Raphaël Berthier</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Raphaël Berthier</h1>
</div>
<table class="imgtable"><tr><td>
<img src="photo4.jpg" alt="" width="350px" />&nbsp;</td>
<td align="left"><p><b>I am hiring a strong theory-oriented PhD student on <a href="proposal.pdf">this project</a>! If interested, feel free to reach out.</b></p>
<h2>Briefly</h2>
<p>I am a tenure-track researcher (chaire de professeur junior) at Inria Sorbonne Université. I am a member of the Inria project-team Megavolt. </p>
<p>Before that, I was a postdoc at EPFL under the supervision of <a href="https://people.epfl.ch/emmanuel.abbe">Emmanuel Abbe</a> and <a href="https://web.stanford.edu/~montanar/">Andrea Montanari</a>. I studied non-convex optimization and the theory of neural networks. </p>
<p>Even before, I was a PhD student at Inria Paris under the supervision of <a href="http://www.di.ens.fr/~fbach/">Francis Bach</a> and <a href="http://pierre.gaillard.me/">Pierre Gaillard</a>. My PhD thesis is a parallel study of convex optimization and gossip algorithms. The best way to learn about this is to look at the summary at the beginning of the <a href="thesis.pdf">manuscript</a>. </p>
<p>Here is a short <a href="cv_english.pdf">CV</a>.</p>
<h2>Contact</h2>
<ul>
<li><p>E-mail: raphael [dot] berthier [at] inria [dot] fr <br /></p>
</li>
<li><p>Physical address: LPSM, Jussieu, Paris. Office 16-26.206. </p>
</li>
</ul>
<h2>Lecture notes </h2>
<p><a href="https://github.com/raphael-berthier/optim-notes/blob/main/notes.pdf">Optimization for machine learning</a>, M2 level</p>
</td></tr></table>
<h2>Publications and Preprints</h2>
<ul>
<li><p>R. Berthier. <b>Diagonal linear networks and the lasso regularization path</b>, 2025, preprint.
<br />[<a href="https://arxiv.org/abs/2509.18766">arXiv</a>, <a href="https://hal.science/hal-05282524">hal</a>] <span class="toggle-trigger">[Show Abstract]<span> 
<div class="toggle-wrap">
<b>Abstract:</b> Diagonal linear networks are neural networks with linear activation and diagonal weight matrices. Their theoretical interest is that their implicit regularization can be rigorously analyzed: from a small initialization, the training of diagonal linear networks converges to the linear predictor with minimal 1-norm among minimizers of the training loss. In this paper, we deepen this analysis showing that the full training trajectory of diagonal linear networks is closely related to the lasso regularization path. In this connection, the training time plays the role of an inverse regularization parameter. Both rigorous results and simulations are provided to illustrate this conclusion. Under a monotonicity assumption on the lasso regularization path, the connection is exact while in the general case, we show an approximate connection. 
</div></p>
</li>
</ul>
<ul>
<li><p>P. Marion, R. Berthier, G. Biau, C. Boyer. <b>Attention layers provably solve single-location regression</b>, 2025, <i>International Conference on Learning Representations (ICLR)</i>. 
<br />[<a href="https://proceedings.iclr.cc/paper_files/paper/2025/hash/dc2c9cd7cb12592856d82dc28ebaab94-Abstract-Conference.html">conference version</a>, <a href="https://cnrs.hal.science/LPSM/hal-04720799v1">hal</a>, <a href="https://arxiv.org/abs/2410.01537">arXiv</a>] <span class="toggle-trigger">[Show Abstract]<span> 
<div class="toggle-wrap">
<b>Abstract:</b> Attention-based models, such as Transformer, excel across various tasks but lack a comprehensive theoretical understanding, especially regarding token-wise sparsity and internal linear representations. To address this gap, we introduce the single-location regression task, where only one token in a sequence determines the output, and its position is a latent random variable, retrievable via a linear projection of the input. To solve this task, we propose a dedicated predictor, which turns out to be a simplified version of a non-linear self-attention layer. We study its theoretical properties, by showing its asymptotic Bayes optimality and analyzing its training dynamics. In particular, despite the non-convex nature of the problem, the predictor effectively learns the underlying structure. This work highlights the capacity of attention mechanisms to handle sparse token information and internal linear structures. 
</div></p>
</li>
</ul>
<ul>
<li><p>D. Pushkin, R. Berthier, E. Abbe. <b>On the minimal degree bias in generalization on the unseen for non-boolean functions</b>, 2024, <i>International Conference on Machine Learning (ICML)</i>. <br />[<a href="https://proceedings.mlr.press/v235/pushkin24a.html">conference version</a>, <a href="https://hal.science/hal-04619375">hal</a>, <a href="https://arxiv.org/abs/2406.06354">arXiv</a>] <span class="toggle-trigger">[Show Abstract]<span> 
<div class="toggle-wrap">
<b>Abstract:</b> We investigate the out-of-domain generalization of random feature (RF) models and Transformers. We first prove that in the &lsquo;generalization on the unseen (GOTU)&rsquo; setting, where training data is fully seen in some part of the domain but testing is made on another part, and for RF models in the small feature regime, the convergence takes place to interpolators of minimal degree as in the Boolean case (Abbe et al., 2023). We then consider the sparse target regime and explain how this regime relates to the small feature regime, but with a different regularization term that can alter the picture in the non-Boolean case. We show two different outcomes for the sparse regime with q-ary data tokens: (1) if the data is embedded with roots of unities, then a min-degree interpolator is learned like in the Boolean case for RF models, (2) if the data is not embedded as such, e.g., simply as integers, then RF models and Transformers may not learn minimal degree interpolators. This shows that the Boolean setting and its roots of unities generalization are special cases where the minimal degree interpolator offers a rare characterization of how learning takes place. For more general integer and real-valued settings, a more nuanced picture remains to be fully characterized. 
</div></p>
</li>
</ul>
<ul>
<li><p>P. Marion, R. Berthier. <b>Leveraging the two timescale regime to demonstrate convergence of neural networks</b>, 2023, <i>Advances in Neural Information Processing Systems (NeurIPS)</i>. <br />[<a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/cd062f8003e38f55dcb93df55b2683d6-Abstract-Conference.html">conference version</a>, <a href="http://arxiv.org/abs/2304.09576">arXiv</a>] <span class="toggle-trigger">[Show Abstract]<span> 
<div class="toggle-wrap">
<b>Abstract:</b> We study the training dynamics of shallow neural networks, in a two-timescale regime in which the stepsizes for the inner layer are much smaller than those for the outer layer. In this regime, we prove convergence of the gradient flow to a global optimum of the non-convex optimization problem in a simple univariate setting. The number of neurons need not be asymptotically large for our result to hold, distinguishing our result from popular recent approaches such as the neural tangent kernel or mean-field regimes. Experimental illustration is provided, showing that the stochastic gradient descent behaves according to our description of the gradient flow and thus converges to a global optimum in the two-timescale regime, but can fail outside of this regime. 
</div></p>
</li>
</ul>
<ul>
<li><p>R. Berthier, A. Montanari, K. Zhou. <b>Learning time-scales in two-layers neural networks</b>, 2023, <i>Foundations of Computational Mathematics (FoCM)</i>. <br />[<a href="https://link.springer.com/article/10.1007/s10208-024-09664-9">journal</a>, <a href="https://arxiv.org/abs/2303.00055">arXiv</a>] <span class="toggle-trigger">[Show Abstract]<span> 
<div class="toggle-wrap">
<b>Abstract:</b> Gradient-based learning in multi-layer neural networks displays a number of striking features. In particular, the decrease rate of empirical risk is non-monotone even after averaging over large batches. Long plateaus in which one observes barely any progress alternate with intervals of rapid decrease. These successive phases of learning often take place on very different time scales. Finally, models learnt in an early phase are typically &lsquo;simpler&rsquo; or &lsquo;easier to learn&rsquo; although in a way that is difficult to formalize.
Although theoretical explanations of these phenomena have been put forward, each of them captures at best certain specific regimes. In this paper, we study the gradient flow dynamics of a wide two-layer neural network in high-dimension, when data are distributed according to a single-index model (i.e., the target function depends on a one-dimensional projection of the covariates). Based on a mixture of new rigorous results, non-rigorous mathematical derivations, and numerical simulations, we propose a scenario for the learning dynamics in this setting. In particular, the proposed evolution exhibits separation of timescales and intermittency. These behaviors arise naturally because the population gradient flow can be recast as a singularly perturbed dynamical system. 
</div></p>
</li>
</ul>
<ul>
<li><p>R. Berthier. <b>Incremental learning in diagonal linear networks</b>, 2023, <i>Journal of Machine Learning Research (JMLR)</i>. <br />[<a href="https://jmlr.org/papers/v24/22-1395.html">journal</a>, <a href="https://arxiv.org/abs/2208.14673">arXiv</a>] <span class="toggle-trigger">[Show Abstract]<span> 
<div class="toggle-wrap">
<b>Abstract:</b> Diagonal linear networks (DLNs) are a toy simplification of artificial neural networks; they consist in a quadratic reparametrization of linear regression inducing a sparse implicit regularization. In this paper, we describe the trajectory of the gradient flow of DLNs in the limit of small initialization. We show that incremental learning is effectively performed in the limit: coordinates are successively activated, while the iterate is the minimizer of the loss constrained to have support on the active coordinates only. This shows that the sparse implicit regularization of DLNs decreases with time. This work is restricted to the underparametrized regime with anti-correlated features for technical reasons. 
</div></p>
</li>
</ul>
<ul>
<li><p>R. Berthier, M. B. Li. <b>Acceleration of gossip algorithms through the Euler-Poisson-Darboux equation</b>, 2022, <i>the IMA Journal of Applied Mathematics</i>. <br />[<a href="https://academic.oup.com/imamat/advance-article-abstract/doi/10.1093/imamat/hxac029/6775269?utm_source=advanceaccess&amp;utm_campaign=imamat&amp;utm_medium=email">journal</a>, <a href="https://arxiv.org/abs/2202.10742">arXiv</a>] <span class="toggle-trigger">[Show Abstract]<span> 
<div class="toggle-wrap">
<b>Abstract:</b> Gossip algorithms and their accelerated versions have been studied exclusively in discrete time on graphs. In this work, we take a different approach, and consider the scaling limit of gossip algorithms in both large graphs and large number of iterations. These limits lead to well-known partial differential equations (PDEs) with insightful properties. On lattices, we prove that the non-accelerated gossip algorithm of Boyd et al. [2006] converges to the heat equation, and the accelerated Jacobi polynomial iteration of Berthier et al. [2020] converges to the Euler-Poisson-Darboux (EPD) equation - a damped wave equation. Remarkably, with appropriate parameters, the fundamental solution of the EPD equation has the ideal gossip behaviour: a uniform density over an ellipsoid, whose radius increases at a rate proportional to t - the fastest possible rate for locally communicating gossip algorithms. This is in contrast with the heat equation where the density spreads on a typical scale of <img class="eq" src="eqs/2909845988863612064-130.png" alt="sqrt{t}" style="vertical-align: -4px" />. Additionally, we provide simulations demonstrating that the gossip algorithms are accurately approximated by their limiting PDEs. 
</div></p>
</li>
</ul>
<ul>
<li><p>C. Gerbelot, R. Berthier. <b>Graph-based approximate message passing iterations</b>, 2021, <i>Information and Inference: a Journal of the IMA</i>. <br />[<a href="https://academic.oup.com/imaiai/article-abstract/12/4/2562/7279192">journal</a>, <a href="https://arxiv.org/abs/2109.11905">arXiv</a>] <span class="toggle-trigger">[Show Abstract]<span> 
<div class="toggle-wrap">
<b>Abstract:</b> Approximate-message passing (AMP) algorithms have become an important element of high-dimensional statistical inference, mostly due to their adaptability and concentration properties, the state evolution (SE) equations. This is demonstrated by the growing number of new iterations proposed for increasingly complex problems, ranging from multi-layer inference to low-rank matrix estimation with elaborate priors. In this paper, we address the following questions: is there a structure underlying all AMP iterations that unifies them in a common framework? Can we use such a structure to give a modular proof of state evolution equations, adaptable to new AMP iterations without reproducing each time the full argument ? We propose an answer to both questions, showing that AMP instances can be generically indexed by an oriented graph. This enables to give a unified interpretation of these iterations, independent from the problem they solve, and a way of composing them arbitrarily. We then show that all AMP iterations indexed by such a graph admit rigorous SE equations, extending the reach of previous proofs, and proving a number of recent heuristic derivations of those equations. Our proof naturally includes non-separable functions and we show how existing refinements, such as spatial coupling or matrix-valued variables, can be combined with our framework. 
</div></p>
</li>
</ul>
<ul>
<li><p>M. Even, R. Berthier, F. Bach, N. Flammarion, P. Gaillard, H. Hendrikx, L. Massoulié, A. Taylor. <b>A continuized view on Nesterov acceleration for stochastic gradient descent and randomized gossip</b>, 2021, oustanding paper award and oral at <i>Advances in Neural Information Processing Systems (NeurIPS)</i>. <br />[<a href="https://proceedings.neurips.cc/paper/2021/hash/ec26fc2eb2b75aece19c70392dc744c2-Abstract.html">conference version</a>, <a href="https://arxiv.org/abs/2106.07644">arXiv</a>, <a href="https://francisbach.com/continuized-acceleration/">an introductory blog post</a>] <span class="toggle-trigger">[Show Abstract]<span> 
<div class="toggle-wrap">
<b>Abstract:</b> We introduce the continuized Nesterov acceleration, a close variant of Nesterov acceleration whose variables are indexed by a continuous time parameter. The two variables continuously mix following a linear ordinary differential equation and take gradient steps at random times. This continuized variant benefits from the best of the continuous and the discrete frameworks: as a continuous process, one can use differential calculus to analyze convergence and obtain analytical expressions for the parameters; and a discretization of the continuized process can be computed exactly with convergence rates similar to those of Nesterov original acceleration. We show that the discretization has the same structure as Nesterov acceleration, but with random parameters. We provide continuized Nesterov acceleration under deterministic as well as stochastic gradients, with either additive or multiplicative noise. Finally, using our continuized framework and expressing the gossip averaging problem as the stochastic minimization of a certain energy function, we provide the first rigorous acceleration of asynchronous gossip algorithms. 
</div></p>
</li>
</ul>
<ul>
<li><p>R. Berthier, F. Bach, P. Gaillard. <b>Tight nonparametric convergence rates for stochastic gradient descent under the noiseless linear model</b>, 2020, <i>Advances in Neural Information Processing Systems (NeurIPS)</i>. <br />[<a href="https://papers.nips.cc/paper/2020/hash/1b33d16fc562464579b7199ca3114982-Abstract.html">conference version</a>, <a href="https://hal.archives-ouvertes.fr/hal-02866755">hal</a>, <a href="https://arxiv.org/abs/2006.08212">arXiv</a>] <span class="toggle-trigger">[Show Abstract]<span> 
<div class="toggle-wrap">
<b>Abstract:</b> In the context of statistical supervised learning, the noiseless linear model assumes that there exists a deterministic linear relation <img class="eq" src="eqs/9200006626087324140-130.png" alt="Y = langle theta_*, X rangle" style="vertical-align: -5px" /> between the random output <img class="eq" src="eqs/11392034264-130.png" alt="Y" style="vertical-align: -0px" /> and the random feature vector <img class="eq" src="eqs/1230657311937148082-130.png" alt="Phi(U)" style="vertical-align: -5px" />, a potentially non-linear transformation of the inputs <img class="eq" src="eqs/10880032724-130.png" alt="U" style="vertical-align: -1px" />. We analyze the convergence of single-pass, fixed step-size stochastic gradient descent on the least-square risk under this model. The convergence of the iterates to the optimum <img class="eq" src="eqs/1854661537855120535-130.png" alt="theta_*" style="vertical-align: -3px" /> and the decay of the generalization error follow polynomial convergence rates with exponents that both depend on the regularities of the optimum <img class="eq" src="eqs/1854661537855120535-130.png" alt="theta_*" style="vertical-align: -3px" /> and of the feature vectors <img class="eq" src="eqs/1230657311937148082-130.png" alt="Phi(U)" style="vertical-align: -5px" />. We interpret our result in the reproducing kernel Hilbert space framework; as a special case, we analyze an online algorithm for estimating a real function on the unit interval from the noiseless observation of its value at randomly sampled points. The convergence depends on the Sobolev smoothness of the function and of a chosen kernel. Finally, we apply our analysis beyond the supervised learning setting to obtain convergence rates for the averaging process (a.k.a. gossip algorithm) on a graph depending on its spectral dimension. 
</div></p>
</li>
</ul>
<ul>
<li><p>R. Berthier, F. Bach, P. Gaillard. <b>Accelerated gossip in networks of given dimension using Jacobi polynomial iterations</b>, 2020, <i>SIAM Journal on Mathematics of Data Science (SIMODS)</i>. <br />[<a href="papers/accelerated_gossip_merged.pdf">journal</a>, <a href="https://hal.archives-ouvertes.fr/hal-01797016/">hal</a>, <a href="https://arxiv.org/abs/1805.08531">arXiv</a>] <span class="toggle-trigger">[Show Abstract]<span> 
<div class="toggle-wrap">
<b>Abstract:</b> Consider a network of agents connected by communication links, where each agent holds a real value. The gossip problem consists in estimating the average of the values diffused in the network in a distributed manner. We develop a method solving the gossip problem that depends only on the spectral dimension of the network, that is, in the communication network set-up, the dimension of the space in which the agents live. This contrasts with previous work that required the spectral gap of the network as a parameter, or suffered from slow mixing. Our method shows an important improvement over existing algorithms in the non-asymptotic regime, i.e., when the values are far from being fully mixed in the network. Our approach stems from a polynomial-based point of view on gossip algorithms, as well as an approximation of the spectral measure of the graphs with a Jacobi measure. We show the power of the approach with simulations on various graphs, and with performance guarantees on graphs of known spectral dimension, such as grids and random percolation bonds. An extension of this work to distributed Laplacian solvers is discussed. As a side result, we also use the polynomial-based point of view to show the convergence of the message passing algorithm for gossip of Moallemi & Van Roy on regular graphs. The explicit computation of the rate of the convergence shows that message passing has a slow rate of convergence on graphs with small spectral gap. 
</div></p>
</li>
</ul>
<ul>
<li><p>R. Berthier, A. Montanari, P.-M. Nguyen. <b>State evolution for approximate message passing with non-separable functions</b>, 2017, <i>Information and Inference: a Journal of the IMA</i>. <br />[<a href="https://doi.org/10.1093/imaiai/iay021">journal</a>, <a href="https://arxiv.org/abs/1708.03950">arXiv</a>] <span class="toggle-trigger">[Show Abstract]<span> 
<div class="toggle-wrap">
<b>Abstract:</b> Given a high-dimensional data matrix <img class="eq" src="eqs/4536482735675098617-130.png" alt="A in {rm I!R}^{n times m}" style="vertical-align: -1px" />, Approximate Message Passing (AMP) algorithms construct sequences of vectors <img class="eq" src="eqs/4353495100622012351-130.png" alt="u^t in {rm I!R}^n" style="vertical-align: -1px" />, <img class="eq" src="eqs/1188365746961071671-130.png" alt="v^t in {rm I!R}^m" style="vertical-align: -1px" />, indexed by <img class="eq" src="eqs/4718066532391173994-130.png" alt="t in {0,1,2,dots }" style="vertical-align: -5px" /> by iteratively applying <img class="eq" src="eqs/8320025024-130.png" alt="A" style="vertical-align: -0px" /> or <img class="eq" src="eqs/593367982112446592-130.png" alt="A^T" style="vertical-align: -0px" />, and suitable non-linear functions, which depend on the specific application. Special instances of this approach have been developed &ndash;among other applications&ndash; for compressed sensing reconstruction, robust regression, Bayesian estimation, low-rank matrix recovery, phase retrieval, and community detection in graphs. For certain classes of random matrices <img class="eq" src="eqs/8320025024-130.png" alt="A" style="vertical-align: -0px" />, AMP admits an asymptotically exact description in the high-dimensional limit <img class="eq" src="eqs/3244372985665555957-130.png" alt="m,nto infty" style="vertical-align: -4px" />, which goes under the name of &lsquo;state evolution.&rsquo;
Earlier work established state evolution for separable non-linearities (under certain regularity conditions). Nevertheless, empirical work demonstrated several important applications that require non-separable functions. In this paper we generalize state evolution to Lipschitz continuous non-separable nonlinearities, for Gaussian matrices <img class="eq" src="eqs/8320025024-130.png" alt="A" style="vertical-align: -0px" />. Our proof makes use of Bolthausen's conditioning technique along with several approximation arguments. In particular, we introduce a modified algorithm (called LAMP for Long AMP) which is of independent interest. 
</div></p>
</li>
</ul>
<h2>PhD thesis</h2>
<ul>
<li><p>R. Berthier. <b>Analysis and acceleration of gradient descents and gossip algorithms</b>, 2021, <a href="thesis.pdf">manuscript</a>.</p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2026-02-09 14:56:15 CET, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
<script>
$(document).ready(function() {
    $(".toggle-trigger").click(function() {
        $(this).parent().nextAll('.toggle-wrap').first().slideToggle('slow');
    });
});
</script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-128753599-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
